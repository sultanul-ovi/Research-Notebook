---
icon: book-circle-arrow-right
layout:
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: false
---

# Reading List

### **LLM and GPU Cluster**

1. \[ArXiv 25] Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs
2. \[ArXiv 25] XPUTimer: Anomaly Diagnostics for Divergent LLM Training in GPU Clusters of Thousand-Plus Scale
3. \[ArXiv 25] TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms
4. \[ArXiv 24] LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization
5. \[ArXiv 24] DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency
6. \[e-Energy '24] Hybrid Heterogeneous Clusters Can Lower the Energy Consumption of LLM Inference Workloads
7. \[ISCA '24] Splitwise: Efficient Generative LLM Inference Using Phase Splitting
8. \[ArXiv 24] Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache
9. \[ArXiv 24] Optimizing LLM Inference Clusters for Enhanced Performance and Energy Efficiency
10. \[IEEE Micro 24] The Breakthrough Memory Solutions for Improved Performance on LLM Inference

### **Heterogenous GPU Cluster**

1. \[ArXiv 25] Carbon and Precedence Aware Scheduling for Data Processing Clusters
2. \[ArXiv 24] Adaptive Asynchronous Work-Stealing for distributed load-balancing in heterogeneous systems.
3. \[ArXiv 24] CASPER: Carbon-Aware Scheduling and Provisioning for Distributed Web Services
4. \[EuroSys 24] HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis
5. \[Journal of Supercomputing 24] LBB: load‐balanced batching for efcient distributed learning on heterogeneous GPU cluster
6. \[Journal of Supercomputing 24] Utilization‐prediction‐aware energy optimization approach for heterogeneous GPU clusters
7. \[USENIX 24] Metis: Fast Automatic Distributed Training on Heterogeneous GPUs
8. \[ArXiv 24] Poplar: Efficient Scaling of Distributed DNN Training on Heterogeneous GPU Clusters
9. \[IEEE 23] Hydra: Deadline-Aware and Efficiency-Oriented Scheduling for Deep Learning Jobs on Heterogeneous GPUs
10. \[SC 23] SYnergy: Fine-grained Energy-Efficient Heterogeneous Computing for Scalable Energy Saving

### **Generative & LLMs**

1. \[SOSP '23] Efficient Memory Management for Large Language Model Serving with PagedAttention
2. \[ICML '23] BPipe: Memory-Balanced Pipeline Parallelism for Training Large Language Models
3. \[ICML '23] Fast Inference from Transformers via Speculative Decoding
4. \[ICML '23] FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU
5. \[ASPLOS '23] Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression
6. \[MICRO '22] DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation
7. \[OSDI '22] Orca: A Distributed Serving System for Transformer-Based Language Generation Tasks

### **Serving Systems (& inference acceleration)**

1. \[SOSP '23] Paella: Low-latency Model Serving with Virtualized GPU Scheduling
2. \[OSDI '23] AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving
3. \[EuroSys '23] Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access
4. \[NSDI '23] SHEPHERD: Serving DNNs in the Wild
5. \[ATC '22] Serving Heterogeneous Machine Learning Models on Multi-GPU Servers with Spatio-Temporal Sharing
6. \[OSDI '22] Achieving µs-scale Preemption for Concurrent GPU-accelerated DNN Inferences
7. \[ATC '21] INFaaS: Automated Model-less Inference Serving
8. \[OSDI '20] Serving DNNs like Clockwork: Performance Predictability from the Bottom Up
9. \[ISCA '20] MLPerf Inference Benchmark
10. \[SOSP '19] Nexus: A GPU Cluster Engine for Accelerating DNN-Based Video Analysis
11. \[ISCA '19] MnnFast: a fast and scalable system architecture for memory-augmented neural networks mcompiled by Jeongseob Ahn Last modified: Summer 2023
12. \[EuroSys '19] µLayer: Low Latency On-Device Inference Using Cooperative Single-Layer Acceleration and Processor-Friendly Quantization
13. \[EuroSys '19] GrandSLAm: Guaranteeing SLAs for Jobs in Microservices Execution Frameworks
14. \[OSDI '18] Pretzel: Opening the Black Box of Machine Learning Prediction Serving Systems
15. \[NSDI '17] Clipper: A Low-Latency Online Prediction Serving System

### **Parallelism & Distributed Systems**

1. \[NSDI '23] ARK: GPU-driven Code Execution for Distributed Deep Learning
2. \[OSDI '22] Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization
3. \[EuroSys '22] Varuna: Scalable, Low-cost Training of Massive Deep Learning Models
4. \[SC '21'] Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines
5. \[ICML '21] PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models
6. \[OSDI '20] A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters
7. \[ATC '20] HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism
8. \[NeurIPS '19] GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
9. \[SOSP '19] A Generic Communication Scheduler for Distributed DNN Training Acceleration
10. \[SOSP '19] PipeDream: Generalized Pipeline Parallelism for DNN Training
11. \[EuroSys '19] Parallax: Sparsity-aware Data Parallel Training of Deep Neural Networks
12. \[arXiv '18] Horovod: fast and easy distributed deep learning in TensorFlow
13. \[ATC '17] Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters
14. \[EuroSys '16] STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning
15. \[EuroSys '16] GeePS: Scalable Deep Learning on Distributed GPUs with a GPU-specialized Parameter Server
16. \[OSDI '14] Scaling Distributed Machine Learning with the Parameter Server
17. \[NIPS '12] Large Scale Distributed Deep Networks

### **GPU Cluster Management**

1. \[OSDI '22] Looking Beyond GPUs for DNN Scheduling on Multi-Tenant Clusters
2. \[NSDI '22] MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters
3. \[OSDI '21] Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning
4. \[NSDI '21] Elastic Resource Sharing for Distributed Deep Learning
5. \[OSDI '20] Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads
6. \[OSDI '20] AntMan: Dynamic Scaling on GPU Clusters for Deep Learning
7. \[NSDI '20] Themis: Fair and Efficient GPU Cluster Scheduling
8. \[EuroSys '20] Balancing Efficiency and Fairness in Heterogeneous GPU Clusters for Deep Learning
9. \[NSDI '19] Tiresias: A GPU Cluster Manager for Distributed Deep Learning
10. \[ATC '19] Analysis of Large-Scale Multi-Tenant GPU Clusters for DNN Training Workloads
11. \[OSDI '18] Gandiva: Introspective cluster scheduling for deep learning

### **Memory Management for Machine Learning**

1. \[ASPLOS '23] DeepUM: Tensor Migration and Prefetching in Unified Memory
2. \[ATC '22] Memory Harvesting in Multi-GPU Systems with Hierarchical Unified Virtual Memory
3. \[MobiSys '22] Memory-efficient DNN Training on Mobile Devices
4. \[HPCA '22] Enabling Efficient Large-Scale Deep Learning Training with Cache Coherent Disaggregated Memory Systems
5. \[ASPLOS '20] Capuchin: Tensor-based GPU Memory Management for Deep Learning
6. \[ASPLOS '20] SwapAdvisor: Push Deep Learning Beyond the GPU Memory Limit via Smart Swapping
7. \[ISCA '19] Interplay between Hardware Prefetcher and Page Eviction Policy in CPU-GPU Unified Virtual Memory
8. \[ISCA '18] Gist: Efficient Data Encoding for Deep Neural Network Training
9. \[PPoPP '18] SuperNeurons: Dynamic GPU Memory Management for Training Deep Neural Networks
10. \[MICRO '16] vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient Neural Network Design

### **Scheduling & Resource Management**

1. \[ASPLOS '23] ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning
2. \[arXiv '22] EasyScale: Accuracy-consistent Elastic Training for Deep Learning
3. \[MLSys '22] VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware
4. \[SIGCOMM '22] Multi-resource interleaving for deep learning training
5. \[EuroSys '22] Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning
6. \[ATC '21] Zico: Efficient GPU Memory Sharing for Concurrent DNN Training
7. \[NeurIPS '20] Nimble: Lightweight and Parallel GPU Task Scheduling for Deep Learning
8. \[OSDI' 20] KungFu: Making Training in Distributed Machine Learning Adaptive
9. \[OSDI '20] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications
10. \[MLSys '20] Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications
11. \[SOSP '19] Generic Communication Scheduler for Distributed DNN Training Acceleration
12. \[EuroSys '18] Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters
13. \[HPCA '18] Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective

### **Deep Learning Compiler**

1. \[PLDI '21] DeepCuts: A Deep Learning Optimization Framework for Versatile GPU Workloads
2. \[OSDI '18] TVM: An Automated End-to-End Optimizing Compiler for Deep Learning

### **Hardware Support for ML**

1. \[ISCA '18] A Configurable Cloud-Scale DNN Processor for Real-Time AI
2. \[ISCA '17] In-Datacenter Performance Analysis of a Tensor Processing Unit

### **Deep Learning Recommendation Models**

1. \[SOSP '23] Bagpipe: Accelerating Deep Recommendation Model Training
2. \[OSDI '22] FAERY: An FPGA-accelerated Embedding-based Retrieval System
3. \[OSDI '22] Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update
4. \[EuroSys '22] Fleche: An Efficient GPU Embedding Cache for Personalized Recommendations
5. \[ASPLOS '22] RecShard: statistical feature-based memory optimization for industry-scale neural recommendation
6. \[HPCA '22] Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized Recommendation
7. \[MLSys '21] TT-Rec: Tensor Train Compression for Deep Learning Recommendation Model Embeddings
8. \[HPCA '21] Tensor Casting: Co-Designing Algorithm-Architecture for Personalized Recommendation Training
9. \[HPCA '21] Understanding Training Efficiency of Deep Learning Recommendation Models at Scale
10. \[ISCA '20] DeepRecSys: A System for Optimizing End-To-End At-scale Neural Recommendation Inference
11. \[HPCA '20] The Architectural Implications of Facebook’s DNN-based Personalized Recommendation
12. \[MICRO '19] TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning

### **ML at Mobile & Embedded Systems**

1. \[MobiCom '20] SPINN: Synergistic Progressive Inference of Neural Networks over Device and Cloud
2. \[RTSS '19] Pipelined Data-Parallel CPU/GPU Scheduling for Multi-DNN Real-Time Inference
3. \[ASPLOS '17] Neurosurgeon: Collaborative Intelligence Between the Cloud and Mobile Edge

### **ML Techniques for Systems**

1. \[ICML '20] An Imitation Learning Approach for Cache Replacement
2. \[ICML '18] Learning Memory Access Patterns

### **Frameworks**

1. \[VLDB '20] PyTorch Distributed: Experiences on Accelerating Data Parallel Training
2. \[NeurIPS '19] PyTorch: An Imperative Style, High-Performance Deep Learning Library
3. \[OSDI '18] Ray: A Distributed Framework for Emerging AI Applications
4. \[OSDI '16] TensorFlow: A System for Large-Scale Machine Learning
